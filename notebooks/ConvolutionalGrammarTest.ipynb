{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CONVOLUTIONAL GRAMMAR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convolutional grammar is based on ConvGrammarV2 and ConvTranslator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GENERATE A RANDOM NETWORK\n",
    "\n",
    "\n",
    "This grammar works with a set of production rules which generate a random network without specifying dimensions such as number of features or kernel size.\n",
    "\n",
    "The user must specify the desired number of layers in the networks and the minimum number of spatial (convolutional) layers.\n",
    "\n",
    "Tree construction is sequential and follows these simple rules:\n",
    "1) First node is always \\<start\\>.\n",
    "2) If previous node is \\<start\\>, sampled node can only be convolutional.\n",
    "3) If previous node is convolutional and the minimum number of spatial layers have not been reached, sampled node can only be convolutional.\n",
    "4) If previous node is convolutional and minimum number of spatial layers is reached, sampled node can be, with the same probability, convolutional or flatten.\n",
    "5) If previous node is flatten sampled node can only be linear.\n",
    "6) If previous node is linear, sampled node can be, with same probability, linear or dropout.\n",
    "7) If previous node is dropout, sampled node can only be linear\n",
    "8) Last node is always \\<end\\>.\n",
    "9) If there is no flattening between a convolution and ending node, a flattening is added."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['conv2d', None, None]\n",
      "['flatten', None, None]\n",
      "['linear', None, None]\n",
      "['<end>', None, None]\n"
     ]
    }
   ],
   "source": [
    "from ConvGrammarV2 import ImageProductionRules\n",
    "\n",
    "n_layers = 15\n",
    "min_spatial_layers = 7\n",
    "\n",
    "production_rules = ImageProductionRules(n_layers = n_layers,\n",
    "                                        min_spatial_layers = min_spatial_layers)\n",
    "\n",
    "empty_tree = production_rules.grow_tree()\n",
    "\n",
    "for node in empty_tree:\n",
    "    print(node)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FILL THE RANDOM NETWORK\n",
    "\n",
    "Class __ImageGrammar__ does two things:\n",
    "    1) Generates an empty tree as before\n",
    "    2) Fill empty tree entries with reasonable values\n",
    "\n",
    "It needs as parameters:\n",
    "- input dimension (ex. (64,64))\n",
    "- input channels (ex. 3 is RGB)\n",
    "- output dimension (ex. 1 could be regression or binary classification)\n",
    "- number of layers\n",
    "- minumum number of spatial layers\n",
    "- hidden\\_in (the number of output channels specified for the first convolution)\n",
    "- hidden\\_out (the number of output channels specified for the last convolution)\n",
    "- shrinkage objective (the dimension the output image from spatial layers should have before flattening, ex. (4,4))\n",
    "\n",
    "Setting layers parameters follows some rules:\n",
    "\n",
    "__\\<start\\>__: this node is filled in the first position with the number of _channels_ of input image and on the second with image _input dimensions_\n",
    "\n",
    "__spatial layers (conv2d)__: these layers need the specification of _hidden\\_in_ and _hidden\\_out_. Each convolution output features number is calculated such that there is a linear convergence from _hidden\\_in_ to _hidden\\_out_. There is also the need to specify the _shrinkage\\_objective_ which is the dimension reduction we would like to reach with convolutional layers before flattening. On the first position of each layer we can find the number of output features while on the second dimension kernel size. Note kernel size is calculated such that each node has almost the same dimension.\n",
    "\n",
    "__flatten__: flattens all the dimensions (except batch dimension) in order to transform last convolutional output image into a single vector. On the first position of each layer we can find _number of features_ resulting from flattening while second position is empty.\n",
    "\n",
    "__linear__: linear modules number of features are set such that there is linear convergence from flattening dimension to objective dimension. In first position we can find number of _output neurons_ while second dimension is empty.\n",
    "\n",
    "__dropout__: _dropout percentage_ is randomly sampled from a Beta with most of the probability mass concentrated in values lower than 0.5. This percenage can be found in first position while second position is empty\n",
    "\n",
    "__\\<end\\>__: this node is the final dense layer, which outputs objective number of features. This quantity is found in first position while second is empty."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start>', 3, (64, 64))\n",
      "('conv2d', 128, (7, 7))\n",
      "('conv2d', 122, (7, 7))\n",
      "('conv2d', 115, (7, 7))\n",
      "('conv2d', 109, (7, 7))\n",
      "('conv2d', 102, (7, 7))\n",
      "('conv2d', 96, (7, 7))\n",
      "('conv2d', 90, (7, 7))\n",
      "('conv2d', 83, (7, 7))\n",
      "('conv2d', 77, (5, 5))\n",
      "('conv2d', 70, (5, 5))\n",
      "('conv2d', 64, (5, 5))\n",
      "('flatten', 1024, None)\n",
      "('linear', 768, None)\n",
      "('linear', 512, None)\n",
      "('dropout', 0.09147014725415402, None)\n",
      "('<end>', 1, None)\n"
     ]
    }
   ],
   "source": [
    "from ConvGrammarV2 import ImageGrammar\n",
    "\n",
    "imgram = ImageGrammar(input_dim=(64,64),\n",
    "                       channels=3,\n",
    "                       output_dim=1,\n",
    "                       n_layers=15,\n",
    "                       min_spatial_layers=10,\n",
    "                       hidden_in=128,\n",
    "                       hidden_out=64,\n",
    "                       shrinkage_objective=(4,4))\n",
    "\n",
    "net = imgram.produceNetwork()\n",
    "for layer in net:\n",
    "    print(layer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TRANSLATE THE MODEL INTO A TORCH WORKING MODEL\n",
    "\n",
    "In order to translate previous network to a working pytorch architecture, we need to pass it to TranslatedNetwork, which only needs the specification of the last activation and the type of all inner activations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 128, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(128, 122, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(122, 115, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): Conv2d(115, 109, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (7): ReLU()\n",
      "  (8): Conv2d(109, 102, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (9): ReLU()\n",
      "  (10): Conv2d(102, 96, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (11): ReLU()\n",
      "  (12): Conv2d(96, 90, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (13): ReLU()\n",
      "  (14): Conv2d(90, 83, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (15): ReLU()\n",
      "  (16): Conv2d(83, 77, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (17): ReLU()\n",
      "  (18): Conv2d(77, 70, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (19): ReLU()\n",
      "  (20): Conv2d(70, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (21): ReLU()\n",
      "  (22): Flatten(start_dim=1, end_dim=-1)\n",
      "  (23): Linear(in_features=1024, out_features=768, bias=True)\n",
      "  (24): ReLU()\n",
      "  (25): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (26): ReLU()\n",
      "  (27): Dropout(p=0.09147014725415402, inplace=False)\n",
      "  (28): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (29): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from ConvTranslator import TranslatedNetwork\n",
    "from torch import nn\n",
    "\n",
    "model = TranslatedNetwork(network_tree=net,\n",
    "                          default_activation=nn.ReLU(),\n",
    "                          default_final_activation=nn.Sigmoid())\n",
    "\n",
    "print(model.model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WORKING EXAMPLE - SYNTHETIC DATA, 50 RANDOM NETWORKS EVALUATION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_samples_per_group = 100\n",
    "train_size = 70\n",
    "\n",
    "mu_group_1 = 1 # all gaussians have mean 1\n",
    "sigma_group_1 = 4 # independent gaussians with variance 4\n",
    "group_1_data = torch.randn((n_samples_per_group, 3, 32, 32)) * sigma_group_1 + mu_group_1\n",
    "train_1_data, train_1_targets = group_1_data[:train_size], torch.ones((train_size, 1))\n",
    "test_1_data, test_1_targets = group_1_data[train_size:], torch.ones((n_samples_per_group - train_size, 1))\n",
    "\n",
    "mu_group_2 = -1 # all gaussians have mean -1\n",
    "sigma_group_2 = 2 # independent gaussians with variance 2\n",
    "group_2_data = torch.randn((n_samples_per_group, 3, 32, 32)) * sigma_group_2 + mu_group_2\n",
    "train_2_data, train_2_targets = group_2_data[:train_size], torch.zeros((train_size, 1))\n",
    "test_2_data, test_2_targets = group_2_data[train_size:], torch.zeros((n_samples_per_group - train_size, 1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_data = torch.cat([train_1_data, train_2_data], dim=0)\n",
    "train_targets = torch.cat([train_1_targets, train_2_targets], dim=0)\n",
    "shuffle_train_index = torch.randperm(n=train_size*2)\n",
    "train_data = train_data[shuffle_train_index]\n",
    "train_targets = train_targets[shuffle_train_index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "test_data = torch.cat([test_1_data, test_2_data], dim=0)\n",
    "test_targets = torch.cat([test_1_targets, test_2_targets], dim=0)\n",
    "shuffle_test_index = torch.randperm(n=n_samples_per_group*2 - train_size*2)\n",
    "test_data = test_data[shuffle_test_index]\n",
    "test_targets = test_targets[shuffle_test_index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "imgram = ImageGrammar(input_dim=(32,32),\n",
    "                       channels=3,\n",
    "                       output_dim=1,\n",
    "                       n_layers=10,\n",
    "                       min_spatial_layers=5,\n",
    "                       hidden_in=128,\n",
    "                       hidden_out=64,\n",
    "                       shrinkage_objective=(4,4))\n",
    "\n",
    "default_activation = nn.ReLU()\n",
    "default_final_activation = nn.Sigmoid()\n",
    "\n",
    "networks = []\n",
    "for i in range(50):\n",
    "    network_tree = imgram.produceNetwork()\n",
    "    networks.append(TranslatedNetwork(network_tree=network_tree,\n",
    "                                      default_activation=default_activation,\n",
    "                                      default_final_activation=default_final_activation))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from Trainer import AutoTrainer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "auto_trainer = AutoTrainer(train_data=train_data,\n",
    "                           train_labels=train_targets,\n",
    "                           test_data=test_data,\n",
    "                           test_labels=test_targets,\n",
    "                           criterion=nn.BCELoss(reduction='sum'),\n",
    "                           optimizer=AdamW,\n",
    "                           num_epochs=100,\n",
    "                           lr=0.01,\n",
    "                           batch_size=64)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nfor network in networks:\\n    avg_test_loss, avg_train_loss, test_accuracy = auto_trainer.train(network)\\n    performance_list['avg_train_loss'].append(avg_train_loss)\\n    performance_list['avg_test_loss'].append(avg_test_loss)\\n    performance_list['test_accuracy'].append(test_accuracy)\\n    \""
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_list = {'avg_train_loss' : [],\n",
    "                    'avg_test_loss' : [],\n",
    "                   'test_accuracy' : []}\n",
    "\n",
    "# RUNNING THIS CODE ON CPU IS NOT EFFICIENT\n",
    "\"\"\"\n",
    "for network in networks:\n",
    "    avg_test_loss, avg_train_loss, test_accuracy = auto_trainer.train(network)\n",
    "    performance_list['avg_train_loss'].append(avg_train_loss)\n",
    "    performance_list['avg_test_loss'].append(avg_test_loss)\n",
    "    performance_list['test_accuracy'].append(test_accuracy)\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
